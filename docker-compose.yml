services:
#RAG Application 
  rag-app:
    build:
      context: .
      dockerfile: Dockerfile.rag-app
    image: rag-app-image
    container_name: rag-app
    env_file:
      - ./.env
    ports:
      - "8006:8000"
    volumes:
      - .:/rag-app
      - /shared/models:/models
    environment:
      API_KEY: ${API_KEY}
      TEI_URL: ${TEI_SERVICE_URL}
      HF_TOKEN: "${HF_TOKEN}"
      CHROMA_HOST: "chromadb_service"
      CHROMA_PORT: "8000"
      LANGSMITH_TRACING_V2: ${LANGSMITH_TRACING_V2}
      LANGSMITH_API_KEY: ${LANGSMITH_API_KEY}
      LANGSMITH_PROJECT: "summaries-rag-app"
    depends_on:
      - tei_service
      - chromadb_service
      - tgi_service
      - sql_db
    working_dir: /rag-app
    # command: ["/bin/bash"]
    # entrypoint: ["/bin/bash", "-c", "uvicorn app.main:app --host 0.0.0.0 --port 8000"]
    stdin_open: true
    tty: true


  # Text Embeddings Inference (TEI) Server
  tei_service:
    image: ghcr.io/huggingface/text-embeddings-inference:hpu-latest # Use Hugging Face's TEI image
    #image: ghcr.io/huggingface/text-embeddings-inference:cpu-ipex-latest
    container_name: tei_service
    runtime: habana
    volumes:
      - /shared/models:/models # Mount shared models for TEI to access embedding models
    environment:
      MODEL_ID: BAAI/bge-small-en-v1.5 #embedding model for TEI to load OR could be in shared/models/hub
      HABANA_VISIBLE_DEVICES: "all"
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      MAX_WARMUP_SEQUENCE_LENGTH: "512"
      #DEVICE: hpu
      PAYLOAD_LIMIT: "100000000" # 100MB (in bytes)
    ports:
      - "8004:80"
    cap_add:
      - SYS_NICE
    ipc: host
    restart: "no"
    
#Text Generation Inference (TGI)
  tgi_service:
    image: ghcr.io/huggingface/tgi-gaudi:2.3.1
    #image: ghcr.io/huggingface/text-generation-inference:3.3.4-intel-cpu
    container_name: tgi_service
    runtime: habana
    volumes: 
      - /shared/models:/models
    # volumes: 
    #     - /shared/models/hub/models--m42-health--Llama3-Med42-70B/snapshots/867064e18aad7bf3f4795f20dcb25a1108952543:/models

    environment:
      MODEL_ID: "m42-health/Llama3-Med42-8B" 
      HUGGINGFACE_HUB_CACHE: "/data" 
      #MODEL_ID: "${TGI_MODEL_PATH}"
      #MODEL_ID: "."
      NUM_SHARD: 1
      HABANA_VISIBLE_DEVICES: "all"
      OMPI_MCA_btl_vader_single_copy_mechanism: "none"
      HF_TOKEN: "${HF_TOKEN}"
      MAX_INPUT_TOKENS: "6000"
      MAX_TOTAL_TOKENS: "8000"
    ports:
      - "8005:80"
    cap_add:
      - SYS_NICE
    ipc: host
    restart: "no"

  #ChromaDB Vector
  chromadb_service:
    image: chromadb/chroma:latest
    container_name: chromadb_service
    ports:
      - "8003:8000"
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
      - ANONYMIZED_TELEMETRY=FALSE
    volumes:
      - chroma_data:/chroma/chroma


  rabbitmq:
    image: rabbitmq:3-management-alpine # Provides a management UI at port 15672
    container_name: rabbitmq_service
    ports:
      - "5673:5672"   # AMQP protocol port for clients
      - "15673:15672" # Management UI (access via http://localhost:15672 with guest/guest)
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    restart: "no"

  rag-rabbitmq-consumer:
    build: 
      context: .
      dockerfile: Dockerfile.rabbitmq-consumer
    image: rag-rabbitmq-consumer-image
    container_name: rag_rabbitmq_consumer_service
    env_file:
      - ./.env
    volumes:
      - .:/rag-app
      - /shared/models:/models
    environment:
      RABBITMQ_HOST: rabbitmq_service
      RABBITMQ_PORT: 5672
      RABBITMQ_USER: guest
      RABBITMQ_PASS: guest
      RABBITMQ_INPUT_QUEUE: rag_prompt
      RABBITMQ_OUTPUT_QUEUE: inference_results
      API_KEY: ${API_KEY}
      TEI_URL: ${TEI_SERVICE_URL}
      HF_TOKEN: "${HF_TOKEN}"
      CHROMA_HOST: "chromadb_service"
      CHROMA_PORT: "8000"
      LANGSMITH_TRACING_V2: ${LANGSMITH_TRACING_V2}
      LANGSMITH_API_KEY: ${LANGSMITH_API_KEY}
    depends_on:
      - rabbitmq
      - tei_service
      - chromadb_service
      - tgi_service
    working_dir: /rag-app
    # entrypoint: ["python3", "app/consumers/rabbitmq_consumer.py"]
    restart: "no"  

  sql_db:
    image: postgres:13-alpine  # Use a lightweight official PostgreSQL image
    container_name: postgres_db
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=mydatabase
    volumes:
      - postgres_data:/var/lib/postgresql/data 

volumes:
  chroma_data:
  rabbitmq_data:
  postgres_data: