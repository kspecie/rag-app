version: '3.8'

services:
  rag-app:
    build:
      context: .
      dockerfile: dockerfile
    container_name: rag_app
    ports:
      - "8000:8000"
    volumes:
      - ./app:/app
      - ./data:/data
      - /shared/models:/models
    environment:
      - TEI_URL=http://tei:80
      - TGI_URL=http://tgi:80
    depends_on:
      - tei
      - tgi
    working_dir: /app
    command: ["python", "app/main.py"]
    stdin_open: true
    tty: true


    # Text Embeddings Inference (TEI) Server (for converting text to numbers)
  tei_server:
    image: ghcr.io/huggingface/text-embeddings-inference:hpu-latest # Use Hugging Face's TEI image
    container_name: tei-service
    ports:
      - "8080:80" # Map server port 8080 to container port 80 for TEI
    volumes:
      - /shared/models:/models # Mount shared models for TEI to access embedding models
    environment:
      # Specify an embedding model for TEI to load. Ensure this model is in /shared/models
      # or will be cached there.
      - MODEL_ID: BAAI/bge-small-en-v1.5 # Example embedding model (you can change this)
      - HF_HOME: /models/huggingface_cache
      - MAX_CONCURRENT_REQUESTS=512
      - MAX_BATCH_TOKENS=16384

      # Add any Gaudi-specific environment variables for TEI from Intel/Habana docs
      # E.g., optimum_habana.transformers.trainer.GaudiTrainer
      # HABANA_VISIBLE_DEVICES=all # Example, check Intel docs


  # Text Generation Inference (TGI) Server (for running Med42 LLM)
  tgi_server:
    image: ghcr.io/huggingface/tgi-gaudi:2.3.1 # Use Hugging Face's TGI image
    container_name: tgi-med42
    ports:
      - "8081:80" # Map server port 8085 to container port 80 for TGI
    volumes:
      - /shared/models:/models # Mount shared models for TGI to access Med42
      - /dev/accel:/dev/accel
    environment:
      # Specify Med42 LLM ID. YOU MUST REPLACE THIS WITH THE CORRECT HUGGING FACE ID FOR MED42
      - MODEL_ID: m42-health/med42-70b
      - HF_HOME: /models/huggingface_cache
      - MAX_CONCURRENT_REQUESTS=512
      - MAX_INPUT_LENGTH=4096
      - MAX_TOTAL_TOKENS=8192
      # Crucial for Gaudi 2 optimization for TGI with Optimum Habana
      # You will need to look up the exact environment variables/flags for TGI with Gaudi.
      # Examples might include:
      # --model-id HuggingFaceH4/zephyr-7b-beta --hostname 0.0.0.0 --port 80 --dtype bfloat16 --num-shard 1 --quantize bitsandbytes --hpu --attn_softmax_bf16 --limit-hpu-graphs --bucket-size 256
      # (These are examples, verify for Med42 and Gaudi 2)